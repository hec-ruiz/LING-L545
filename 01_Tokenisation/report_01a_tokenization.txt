### create japanese dictionary (available @ ~/LING-L545/UD_Japanese-GSD/jp.dict)
# from within the directory where conllu is
# first project on virtual screen all contents with cat
# then find all sentences that start with a number
# cut everything on the second field
# sort by unique
# store in jp.dict which is a text file
cat ~/LING-L545/UD_Japanese-GSD/ja_gsd-ud-train.conllu | grep ^[0-9] | cut -f2 | sort -u > ~/LING-L545/01_Tokenisation/jp.dict

### get the full sentences for the test
# find lines that start with "# text = "
# cut -f2 -d  "=" means cutting the second field using "=" instead of tab as a separator
cat ~/LING-L545/UD_Japanese-GSD/ja_gsd-ud-test.conllu | grep "# text = " | cut -f2 -d "=" | sed 's/^ *//g' > ~/LING-L545/01_Tokenisation/jp.src

### get the list of words in the test
# -v option for grep means inverting match
# cut -f2 means cutting the second field (tab separated file)
# for pure blank lines insert "@@@@" instead
# replace newline with spaces
# replace "@@@@" with newlines
# remove leading blank spaces (for sentences that start with 0 or more white space substitute them with nothing)
# save file 
cat ~/LING-L545/UD_Japanese-GSD/ja_gsd-ud-test.conllu | grep -v "#" | cut -f2 | sed "s/^$/@@@@/g" | tr '\n' ' ' | sed "s/@@@@/\n/g" | sed 's/^ *//' > ~/LING-L545/01_Tokenisation/jp.testwords

### look at the beginning of each file
head jp.dict
head jp.src
head jp.testwords

# get a file of maxmatched sentences
# first argument is the script, second argument is the target file to be tokenized, 
# third argument is the dictionary of words, 4th argument is filename for the result
maxmatch.py jp.src jp.dict "jp.maxmatch"

# get the word error rate of maxmatched sentences vs reference sentences
# the wer.py was taken from https://raw.githubusercontent.com/zszyellow/WER-in-python/master/wer.py
# and modified to make it python3-compatible and provide some summaries of the data
wer.py jp.testwords jp.maxmatch

# Some results:
Average WER =  49.24
Min: 0.000
Q1: 14.286
Median: 27.196
Q3: 50.000
Max: 1466.667

The performance is far from perfect(the ideal WER would be 0), but there are some considerable outliers that seem very challenging to tokenize.
